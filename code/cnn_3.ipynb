{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding, Flatten\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "import utils\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# Performs classification using CNN.\n",
    "\n",
    "FREQ_DIST_FILE = '../twitter_data/3-sentiment-processed-train-freqdist.pkl'\n",
    "BI_FREQ_DIST_FILE = '../twitter_data/3-sentiment-processed-train-freqdist-bi.pkl'\n",
    "TRAIN_PROCESSED_FILE = '../twitter_data/3-sentiment-processed-train.csv'\n",
    "TEST_PROCESSED_FILE = '../twitter_data/3-sentiment-processed-X-test.csv'\n",
    "TEST_LABEL_FILE = '../twitter_data/3-sentiment-processed-y-test.csv'\n",
    "GLOVE_FILE = '../dataset/glove-seeds.txt'\n",
    "REPORT_FILE = './reports/3-sentiments.csv'\n",
    "train = False\n",
    "dim = 200\n",
    "\n",
    "\n",
    "def get_glove_vectors(vocab):\n",
    "    \"\"\"\n",
    "    Extracts glove vectors from seed file only for words present in vocab.\n",
    "    \"\"\"\n",
    "    print('Looking for GLOVE seeds')\n",
    "    glove_vectors = {}\n",
    "    found = 0\n",
    "    with open(GLOVE_FILE, 'r', encoding='utf-8') as glove_file:\n",
    "        for i, line in enumerate(glove_file):\n",
    "            utils.write_status(i + 1, 0)\n",
    "            tokens = line.strip().split()\n",
    "            word = tokens[0]\n",
    "            if vocab.get(word):\n",
    "                vector = [float(e) for e in tokens[1:]]\n",
    "                glove_vectors[word] = np.array(vector)\n",
    "                found += 1\n",
    "    print('\\n')\n",
    "    return glove_vectors\n",
    "\n",
    "\n",
    "def get_feature_vector(tweet):\n",
    "    \"\"\"\n",
    "    Generates a feature vector for each tweet where each word is\n",
    "    represented by integer index based on rank in vocabulary.\n",
    "    \"\"\"\n",
    "    words = tweet.split()\n",
    "    feature_vector = []\n",
    "    for i in range(len(words) - 1):\n",
    "        word = words[i]\n",
    "        if vocab.get(word) is not None:\n",
    "            feature_vector.append(vocab.get(word))\n",
    "    if len(words) >= 1:\n",
    "        if vocab.get(words[-1]) is not None:\n",
    "            feature_vector.append(vocab.get(words[-1]))\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "def process_tweets(csv_file, test_file=True):\n",
    "    \"\"\"\n",
    "    Generates training X, y pairs.\n",
    "    \"\"\"\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    print('Generating feature vectors')\n",
    "    with open(csv_file, 'r', encoding=\"utf-8\") as csv:\n",
    "        lines = csv.readlines()\n",
    "        total = len(lines)\n",
    "        for i, line in enumerate(lines):\n",
    "            if test_file:\n",
    "                tweet_id, tweet = line.split(',')\n",
    "            else:\n",
    "                tweet_id, sentiment, tweet = line.split(',')\n",
    "                # Convert sentiment labels to one-hot encoding\n",
    "                sentiment_onehot = [0, 0, 0]\n",
    "                if sentiment == \"1\":\n",
    "                    sentiment_onehot[2] = 1\n",
    "                elif sentiment == \"0\":\n",
    "                    sentiment_onehot[1] = 1\n",
    "                elif sentiment == \"-1\":\n",
    "                    sentiment_onehot[0] = 1\n",
    "                labels.append(sentiment_onehot)\n",
    "            feature_vector = get_feature_vector(tweet)\n",
    "            if test_file:\n",
    "                tweets.append(feature_vector)\n",
    "            else:\n",
    "                tweets.append(feature_vector)\n",
    "            utils.write_status(i + 1, total)\n",
    "    print('\\n')\n",
    "    return tweets, np.array(labels)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating feature vectors\n",
      "Processing 85224/85224\n",
      "\n",
      "85224 85224\n",
      "Looking for GLOVE seeds\n",
      "Processing 1193517/0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "vocab_size = 90000\n",
    "batch_size = 500\n",
    "max_length = 30\n",
    "filters = 600\n",
    "kernel_size = 3\n",
    "vocab = utils.top_n_words(FREQ_DIST_FILE, vocab_size, shift=1)\n",
    "\n",
    "\n",
    "tweets, labels = process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
    "print(len(tweets), len(labels))\n",
    "glove_vectors = get_glove_vectors(vocab)\n",
    "# Create and embedding matrix\n",
    "embedding_matrix = np.random.randn(vocab_size + 1, dim) * 0.01\n",
    "# Seed it with GloVe vectors\n",
    "for word, i in vocab.items():\n",
    "    glove_vector = glove_vectors.get(word)\n",
    "    if glove_vector is not None:\n",
    "        embedding_matrix[i] = glove_vector\n",
    "tweets = pad_sequences(tweets, maxlen=max_length, padding='post')\n",
    "shuffled_indices = np.random.permutation(tweets.shape[0])\n",
    "tweets = tweets[shuffled_indices]\n",
    "labels = labels[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1  2134     2 ...     0     0     0]\n",
      " [   25   103   308 ...     0     0     0]\n",
      " [   99    10  6838 ...    60     1    47]\n",
      " ...\n",
      " [67025     0     0 ...     0     0     0]\n",
      " [    1    52    14 ...     0     0     0]\n",
      " [   26     1    13 ...     0     0     0]] [[0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " ...\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 3) and (None, 1) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ljh\\Desktop\\msc study\\Machine Learning Practical\\TwitterSA\\code\\cnn_3.ipynb 单元格 3\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ljh/Desktop/msc%20study/Machine%20Learning%20Practical/TwitterSA/code/cnn_3.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m reduce_lr \u001b[39m=\u001b[39m ReduceLROnPlateau(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, factor\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, min_lr\u001b[39m=\u001b[39m\u001b[39m0.000001\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ljh/Desktop/msc%20study/Machine%20Learning%20Practical/TwitterSA/code/cnn_3.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(tweets, labels)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ljh/Desktop/msc%20study/Machine%20Learning%20Practical/TwitterSA/code/cnn_3.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(tweets, labels, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, callbacks\u001b[39m=\u001b[39;49m[checkpoint, reduce_lr])\n",
      "File \u001b[1;32mc:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filepgeu4dp7.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\losses.py\", line 272, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\losses.py\", line 1990, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\ljh\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend.py\", line 5529, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 3) and (None, 1) are incompatible\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size + 1, dim, weights=[embedding_matrix], input_length=max_length))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "filepath = \"./models/lstm-{epoch:02d}-{loss:0.3f}-{val_loss:0.3f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor=\"loss\", verbose=1, save_best_only=True, mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=0.000001)\n",
    "print(model.summary())\n",
    "model.fit(tweets, labels, batch_size=128, epochs=5, validation_split=0.1, shuffle=True, callbacks=[checkpoint, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 30, 200)           18000200  \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 30, 200)           0         \n",
      "                                                                 \n",
      " conv1d_20 (Conv1D)          (None, 28, 600)           360600    \n",
      "                                                                 \n",
      " conv1d_21 (Conv1D)          (None, 26, 300)           540300    \n",
      "                                                                 \n",
      " conv1d_22 (Conv1D)          (None, 24, 150)           135150    \n",
      "                                                                 \n",
      " conv1d_23 (Conv1D)          (None, 22, 75)            33825     \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 1650)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 600)               990600    \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 600)               0         \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 600)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 3)                 1803      \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 3)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,062,478\n",
      "Trainable params: 20,062,478\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Generating feature vectors\n",
      "Processing 21306/21306\n",
      "\n",
      "167/167 [==============================] - 1s 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.95      0.94      7082\n",
      "     neutral       0.97      0.97      0.97      7132\n",
      "    positive       0.96      0.93      0.94      7092\n",
      "\n",
      "    accuracy                           0.95     21306\n",
      "   macro avg       0.95      0.95      0.95     21306\n",
      "weighted avg       0.95      0.95      0.95     21306\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE = './models/4cnn-08-0.080-0.200.hdf5'\n",
    "model = load_model(MODEL_FILE)\n",
    "print(model.summary())\n",
    "test_tweets, _ = process_tweets(TEST_PROCESSED_FILE, test_file=True)\n",
    "test_tweets = pad_sequences(test_tweets, maxlen=max_length, padding='post')\n",
    "predictions = model.predict(test_tweets, batch_size=128, verbose=1)\n",
    "results = np.argmax(predictions, axis=1) - 1 # Convert back to original labels (-1, 0, 1)\n",
    "id_results = zip(map(str, range(len(test_tweets))), results)\n",
    "# utils.save_results_to_csv(id_results, 'cnn.csv')\n",
    "test_label = utils.file_number_to_list(TEST_LABEL_FILE)\n",
    "report = classification_report(test_label, results, target_names=['negative', 'neutral', 'positive'], output_dict=True)\n",
    "print(classification_report(test_label, results, target_names=['negative', 'neutral', 'positive'], output_dict=False))\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "# df_report.to_csv(REPORT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "577c8467bcf36459c25c1347bd50417841062b742516a10be0b28db3fde86541"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
